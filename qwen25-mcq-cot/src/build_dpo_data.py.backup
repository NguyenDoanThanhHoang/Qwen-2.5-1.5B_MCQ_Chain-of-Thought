"""
Build DPO preference pairs from ECQA dataset
Create (prompt, chosen, rejected) triplets for Direct Preference Optimization
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

import json
import random
from typing import Dict, List
from pathlib import Path
from datasets import load_dataset

from configs.config import data_config
from src.prepare_data import load_ecqa_dataset, format_prompt
from src.utils import find_answer_key, has_good_explanation


def load_sft_rejected_samples(split: str) -> dict:
    """
    Load SFT-generated rejected samples from JSONL files

    Args:
        split: Dataset split ('train' or 'validation')

    Returns:
        Dictionary mapping (question, choices_tuple) -> rejected_text
    """
    file_map = {
        'train': 'data/sft_rejected_train.jsonl',
        'validation': 'data/sft_rejected_val.jsonl',
    }

    file_path = file_map.get(split)
    if not file_path or not Path(file_path).exists():
        print(f"  SFT rejected samples not found at: {file_path}")
        print(f"  Will use template-based rejected samples instead.")
        return {}

    print(f"  Loading SFT-generated rejected samples from: {file_path}")
    rejected_map = {}

    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            sample = json.loads(line)
            key = (sample['question'], tuple(sample['choices']))
            rejected_map[key] = sample['rejected_text']

    print(f"  Loaded {len(rejected_map)} SFT-generated rejected samples")
    return rejected_map


def create_dpo_pair(example: Dict, rejected_map: dict = None, use_advanced_rejected: bool = False) -> Dict:
    """
    Create a single DPO preference pair

    Args:
        example: Single example from ECQA dataset
        rejected_map: Optional dictionary of SFT-generated rejected samples
        use_advanced_rejected: Use advanced rejected samples (corrupted reasoning) if rejected_map not available

    Returns:
        Dictionary with prompt, chosen, rejected
    """
    # ECQA dataset structure
    question = example['q_text']
    choices = [example[f'q_op{i}'] for i in range(1, 6)]

    # Find answer key from answer text
    answer_key = find_answer_key(answer_text, choices)

    # Get explanation (ECQA has 'taskB' field)
    explanation = example.get('taskB', '').strip()
    if not explanation:
        explanation = "Let me think step by step about this question."

    # Format prompt (same as SFT)
    prompt = format_prompt(question, choices)

    # CHOSEN: Full explanation + correct answer
    chosen = f"{explanation}\nAnswer: {answer_key}"

    # REJECTED: Try to use SFT-generated rejected sample, fallback to templates
    wrong_choice = choices[ord(wrong_answer) - ord('A')]

    # Strategy 1: Use SFT-generated rejected sample if available
    if rejected_map:
        key = (question, tuple(choices))
        rejected = rejected_map.get(key)

        if rejected:
            # Successfully found SFT-generated rejected sample
            pass  # Use it as-is
        else:
            # Fallback to template if SFT sample not found for this question
            rejected = create_template_rejected(wrong_answer, wrong_choice, use_advanced_rejected, explanation)
    else:
        # Strategy 2: Use template-based rejected samples
        rejected = create_template_rejected(wrong_answer, wrong_choice, use_advanced_rejected, explanation)

    # ADVANCED: Create plausible wrong reasoning (same length as chosen)
    # Strategy: Use partial explanation + wrong conclusion
    explanation_parts = explanation.split('.')
    if len(explanation_parts) > 2:
        # Take first part, add wrong conclusion
        partial_reasoning = '. '.join(explanation_parts[:len(explanation_parts)//2])
        return f"{partial_reasoning}. Therefore, {wrong_choice} is the best answer.\nAnswer: {wrong_answer}"
    else:
        # Fallback: Create reasoning for wrong choice
        rejected_reasoning = f"Considering the question, {wrong_choice} seems like the most appropriate answer because it directly relates to the context."
        return f"{rejected_reasoning}\nAnswer: {wrong_answer}"
else:
    # SIMPLE: Short templates (for comparison baseline)
    rejected_templates = [
        f"I think the answer is {wrong_answer}.\nAnswer: {wrong_answer}",
        f"After considering the options, I believe {wrong_answer} is correct.\nAnswer: {wrong_answer}",
        f"The most reasonable choice appears to be {wrong_answer}.\nAnswer: {wrong_answer}",
        f"Looking at the options, {wrong_answer} seems right.\nAnswer: {wrong_answer}",
    ]
    return random.choice(rejected_templates)


def build_dpo_dataset(split: str = "train", sample_size: int = None, use_sft_rejected: bool = True, use_advanced_rejected: bool = True, filter_bad_explanations: bool = True) -> List[Dict]:
    """
    Build full DPO dataset from ECQA

    Args:
        split: Dataset split to use
        sample_size: Number of samples (None = all)
        use_sft_rejected: Use SFT-generated rejected samples if available (default True)
        use_advanced_rejected: Use improved rejected samples (default True) when SFT samples not available
        filter_bad_explanations: Filter out samples without good explanations (default True)

    Returns:
        List of DPO preference pairs
    """
    # Load raw dataset
    print(f"Loading {split} split for DPO...")
    raw_ds = load_ecqa_dataset(split)

    # Load SFT-generated rejected samples if requested
    rejected_map = None
    if use_sft_rejected:
        rejected_map = load_sft_rejected_samples(split)

    # Set random seed for consistent rejected answers
    random.seed(data_config.seed)

    # Convert to DPO pairs with filtering
    print("Creating DPO preference pairs...")
    dpo_pairs = []
    filtered_count = 0

    for example in raw_ds:
        # FILTER: Skip samples without good explanations
        if filter_bad_explanations:
            if not has_good_explanation(example):
                filtered_count += 1
                continue  # Skip this sample

        pair = create_dpo_pair(example, rejected_map=rejected_map, use_advanced_rejected=use_advanced_rejected)
        dpo_pairs.append(pair)

    if filter_bad_explanations:
        print(f"Filtered out {filtered_count} samples without good explanations")

    # Sample if requested (after filtering)
    if sample_size is not None and sample_size < len(dpo_pairs):
        dpo_pairs = dpo_pairs[:sample_size]
        print(f"Sampled {sample_size} examples")

    return dpo_pairs


def save_dpo_pairs(dpo_pairs: List[Dict], output_file: str = None):
    """
    Save DPO pairs to JSONL file

    Args:
        dpo_pairs: List of preference pairs
        output_file: Output file path
    """
    if output_file is None:
        output_file = data_config.dpo_pairs_file

    # Create directory if needed
    Path(output_file).parent.mkdir(parents=True, exist_ok=True)

    print(f"\nSaving DPO pairs to {output_file}...")

    with open(output_file, 'w', encoding='utf-8') as f:
        for pair in dpo_pairs:
            f.write(json.dumps(pair, ensure_ascii=False) + '\n')

    print(f"Saved {len(dpo_pairs)} DPO pairs ✓")


def validate_dpo_pairs(dpo_pairs: List[Dict], num_samples: int = 2):
    """
    Print sample DPO pairs to validate formatting

    Args:
        dpo_pairs: List of preference pairs
        num_samples: Number of samples to print
    """
    print(f"\n{'='*80}")
    print(f"DPO PAIRS VALIDATION - Showing {num_samples} samples")
    print(f"{'='*80}\n")

    for i in range(min(num_samples, len(dpo_pairs))):
        pair = dpo_pairs[i]

        print(f"--- Sample {i+1} ---")
        print(f"Correct Answer: {pair['answer_key']}")
        print(f"Wrong Answer: {pair['wrong_answer']}")

        print(f"\nPROMPT:\n{pair['prompt'][:300]}...")

        print(f"\nCHOSEN (correct):\n{pair['chosen'][:200]}...")

        print(f"\nREJECTED (wrong):\n{pair['rejected'][:200]}...")

        print(f"\n{'-'*80}\n")


def load_dpo_pairs(file_path: str = None) -> List[Dict]:
    """
    Load DPO pairs from JSONL file

    Args:
        file_path: Path to JSONL file

    Returns:
        List of DPO pairs
    """
    if file_path is None:
        file_path = data_config.dpo_pairs_file

    print(f"Loading DPO pairs from {file_path}...")

    pairs = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            pairs.append(json.loads(line))

    print(f"Loaded {len(pairs)} DPO pairs")
    return pairs


if __name__ == "__main__":
    """Build and save DPO dataset"""

    print("Building DPO preference pairs from ECQA...\n")

    # Build DPO pairs from training set
    print("="*80)
    print("BUILDING TRAINING DPO PAIRS")
    print("="*80)
    dpo_train_pairs = build_dpo_dataset(
        split="train",
        sample_size=data_config.train_sample_size
    )

    print(f"\nGenerated {len(dpo_train_pairs)} training DPO preference pairs")

    # Validate formatting
    validate_dpo_pairs(dpo_train_pairs, num_samples=2)

    # Save training pairs
    save_dpo_pairs(dpo_train_pairs, output_file=data_config.dpo_pairs_file)

    # Build DPO pairs from validation set
    print("\n" + "="*80)
    print("BUILDING VALIDATION DPO PAIRS")
    print("="*80)
    dpo_val_pairs = build_dpo_dataset(
        split="validation",
        sample_size=data_config.val_sample_size
    )

    print(f"\nGenerated {len(dpo_val_pairs)} validation DPO preference pairs")

    # Save validation pairs
    save_dpo_pairs(dpo_val_pairs, output_file=data_config.dpo_val_pairs_file)

    # Verify saved pairs
    print("\n" + "="*80)
    print("VERIFICATION")
    print("="*80)

    loaded_train = load_dpo_pairs(data_config.dpo_pairs_file)
    loaded_val = load_dpo_pairs(data_config.dpo_val_pairs_file)

    print(f"\nTraining pairs:")
    print(f"  Generated: {len(dpo_train_pairs)}")
    print(f"  Saved and loaded: {len(loaded_train)}")
    print(f"  Match: {'✓' if len(dpo_train_pairs) == len(loaded_train) else '✗'}")

    print(f"\nValidation pairs:")
    print(f"  Generated: {len(dpo_val_pairs)}")
    print(f"  Saved and loaded: {len(loaded_val)}")
    print(f"  Match: {'✓' if len(dpo_val_pairs) == len(loaded_val) else '✗'}")

    print("\n✓ DPO data preparation completed!")
