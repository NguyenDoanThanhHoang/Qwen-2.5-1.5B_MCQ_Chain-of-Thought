{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üöÄ Llama 3.2 1B: SFT + DPO Training Pipeline\n",
    "\n",
    "## Full Training Pipeline - Just Click \"Run All\"!\n",
    "\n",
    "### ‚öôÔ∏è Prerequisites:\n",
    "1. ‚úÖ Runtime: **GPU (T4)** - Runtime ‚Üí Change runtime type ‚Üí GPU\n",
    "2. ‚úÖ Upload project folder to: `MyDrive/llama32-mcq-cot/`\n",
    "3. ‚úÖ Get tokens ready:\n",
    "   - HuggingFace token: https://huggingface.co/settings/tokens\n",
    "   - Wandb token: https://wandb.ai/authorize\n",
    "\n",
    "### üìä Expected Timeline:\n",
    "- Setup: ~10 minutes\n",
    "- SFT training: ~3-4 hours\n",
    "- DPO training: ~2-3 hours\n",
    "- Evaluation: ~30 minutes\n",
    "- **Total: ~6-8 hours**\n",
    "\n",
    "### üéØ What You'll Get:\n",
    "- Base model accuracy: ~40-50%\n",
    "- SFT model accuracy: ~55-65%\n",
    "- DPO model accuracy: ~57-68%\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mount"
   },
   "source": [
    "## Step 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_code"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "print(\"üìÅ Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "print(\"‚úì Drive mounted successfully!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "navigate"
   },
   "source": [
    "## Step 2: Navigate to Project Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "navigate_code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_DIR = '/content/drive/MyDrive/llama32-mcq-cot'\n",
    "\n",
    "print(f\"üìÇ Navigating to: {PROJECT_DIR}\")\n",
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "# Verify\n",
    "print(f\"‚úì Current directory: {os.getcwd()}\")\n",
    "print(\"\\nüìÑ Project files:\")\n",
    "!ls -la\n",
    "\n",
    "# Verify required directories exist\n",
    "required_dirs = ['src', 'configs']\n",
    "for dir_name in required_dirs:\n",
    "    if Path(dir_name).exists():\n",
    "        print(f\"‚úì {dir_name}/ found\")\n",
    "    else:\n",
    "        print(f\"‚úó {dir_name}/ NOT FOUND! Please check your upload.\")\n",
    "\n",
    "# Create data directory if needed\n",
    "Path('data').mkdir(exist_ok=True)\n",
    "print(\"‚úì data/ directory ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install"
   },
   "source": [
    "## Step 3: Install Dependencies\n",
    "\n",
    "Installing required packages... (~5 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_code"
   },
   "outputs": [],
   "source": [
    "print(\"üì¶ Installing dependencies...\\n\")\n",
    "\n",
    "!pip install -q transformers>=4.44.0\n",
    "!pip install -q datasets>=2.14.0\n",
    "!pip install -q accelerate>=0.24.0\n",
    "!pip install -q bitsandbytes>=0.41.0\n",
    "!pip install -q peft>=0.6.0\n",
    "!pip install -q trl>=0.7.0\n",
    "!pip install -q wandb>=0.15.0\n",
    "!pip install -q scipy scikit-learn\n",
    "\n",
    "print(\"\\n‚úì All packages installed!\\n\")\n",
    "\n",
    "# Verify installations\n",
    "import transformers\n",
    "import torch\n",
    "import datasets\n",
    "import peft\n",
    "import trl\n",
    "\n",
    "print(\"üìä Package Versions:\")\n",
    "print(f\"  Transformers: {transformers.__version__}\")\n",
    "print(f\"  Datasets: {datasets.__version__}\")\n",
    "print(f\"  PEFT: {peft.__version__}\")\n",
    "print(f\"  TRL: {trl.__version__}\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "\n",
    "print(\"\\nüñ•Ô∏è GPU Info:\")\n",
    "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è WARNING: No GPU detected! Please change runtime to GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auth"
   },
   "source": [
    "## Step 4: Authentication (HuggingFace & Wandb)\n",
    "\n",
    "**You'll need to enter tokens here:**\n",
    "- HuggingFace: https://huggingface.co/settings/tokens\n",
    "- Wandb: https://wandb.ai/authorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auth_hf"
   },
   "outputs": [],
   "source": [
    "# HuggingFace Login\n",
    "print(\"ü§ó HuggingFace Login\")\n",
    "print(\"Get your token from: https://huggingface.co/settings/tokens\\n\")\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auth_wandb"
   },
   "outputs": [],
   "source": [
    "# Wandb Login\n",
    "print(\"üìä Wandb Login\")\n",
    "print(\"Get your token from: https://wandb.ai/authorize\\n\")\n",
    "\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_prep"
   },
   "source": [
    "## Step 5: Data Preparation\n",
    "\n",
    "Loading ECQA dataset and validating format..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_prep_code"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STEP 5: DATA PREPARATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\")\n",
    "\n",
    "!python src/prepare_data.py\n",
    "\n",
    "print(\"\\n‚úì Data preparation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpo_data"
   },
   "source": [
    "## Step 6: Build DPO Preference Pairs\n",
    "\n",
    "Creating (prompt, chosen, rejected) pairs for DPO training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dpo_data_code"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STEP 6: BUILD DPO PREFERENCE PAIRS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\")\n",
    "\n",
    "!python src/build_dpo_data.py\n",
    "\n",
    "print(\"\\n‚úì DPO data created!\")\n",
    "\n",
    "# Verify file exists\n",
    "from pathlib import Path\n",
    "dpo_file = Path('data/dpo_pairs.jsonl')\n",
    "if dpo_file.exists():\n",
    "    import json\n",
    "    with open(dpo_file, 'r') as f:\n",
    "        num_pairs = sum(1 for _ in f)\n",
    "    print(f\"‚úì DPO pairs file: {num_pairs} pairs saved\")\n",
    "else:\n",
    "    print(\"‚úó Warning: DPO pairs file not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sft"
   },
   "source": [
    "## Step 7: SFT Training (Supervised Fine-Tuning)\n",
    "\n",
    "### ‚è±Ô∏è Expected Time: ~3-4 hours\n",
    "\n",
    "Training Llama 3.2 1B with QLoRA on ECQA dataset...\n",
    "\n",
    "**What's happening:**\n",
    "- 4-bit quantization to save memory\n",
    "- LoRA adapters (r=16) for efficient training\n",
    "- Training on ~10k samples\n",
    "- Wandb tracking enabled\n",
    "\n",
    "**‚ö†Ô∏è Don't close browser during training!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sft_code"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STEP 7: SFT TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(\"\")\n",
    "print(\"‚è±Ô∏è  This will take ~3-4 hours\")\n",
    "print(\"üìä Monitor progress on wandb (link will appear below)\")\n",
    "print(\"\")\n",
    "\n",
    "!python src/train_sft.py\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì SFT TRAINING COMPLETED!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sft_verify"
   },
   "outputs": [],
   "source": [
    "# Verify SFT model saved\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\nüîç Verifying SFT outputs...\\n\")\n",
    "\n",
    "sft_merged = Path(\"outputs/sft-llama32-1b-mcq-merged\")\n",
    "if sft_merged.exists():\n",
    "    print(f\"‚úì SFT merged model saved\")\n",
    "    !ls -lh outputs/sft-llama32-1b-mcq-merged/ | head -10\n",
    "else:\n",
    "    print(f\"‚úó SFT merged model not found\")\n",
    "\n",
    "# Clear GPU memory\n",
    "import torch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nüßπ GPU memory cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpo"
   },
   "source": [
    "## Step 8: DPO Training (Direct Preference Optimization)\n",
    "\n",
    "### ‚è±Ô∏è Expected Time: ~2-3 hours\n",
    "\n",
    "Training with preference pairs to improve reasoning quality...\n",
    "\n",
    "**What's happening:**\n",
    "- Loading SFT checkpoint\n",
    "- Training on preference pairs (correct vs wrong reasoning)\n",
    "- Beta=0.1 for preference strength\n",
    "- Lower learning rate than SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dpo_code"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STEP 8: DPO TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(\"\")\n",
    "print(\"‚è±Ô∏è  This will take ~2-3 hours\")\n",
    "print(\"üìä Monitor progress on wandb\")\n",
    "print(\"\")\n",
    "\n",
    "!python src/train_dpo.py\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì DPO TRAINING COMPLETED!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dpo_verify"
   },
   "outputs": [],
   "source": [
    "# Verify DPO model saved\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\nüîç Verifying DPO outputs...\\n\")\n",
    "\n",
    "dpo_merged = Path(\"outputs/dpo-llama32-1b-mcq-merged\")\n",
    "if dpo_merged.exists():\n",
    "    print(f\"‚úì DPO merged model saved\")\n",
    "    !ls -lh outputs/dpo-llama32-1b-mcq-merged/ | head -10\n",
    "else:\n",
    "    print(f\"‚úó DPO merged model not found\")\n",
    "\n",
    "# Clear GPU memory\n",
    "import torch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nüßπ GPU memory cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eval"
   },
   "source": [
    "## Step 9: Evaluation\n",
    "\n",
    "### ‚è±Ô∏è Expected Time: ~20-30 minutes\n",
    "\n",
    "Comparing all three models:\n",
    "1. Base Llama-3.2-1B-Instruct\n",
    "2. SFT model\n",
    "3. DPO model\n",
    "\n",
    "On ECQA validation set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eval_code"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STEP 9: MODEL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\")\n",
    "print(\"‚è±Ô∏è  This will take ~20-30 minutes\")\n",
    "print(\"üéØ Evaluating: Base ‚Üí SFT ‚Üí DPO\")\n",
    "print(\"\")\n",
    "\n",
    "!python src/evaluate.py\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì EVALUATION COMPLETED!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test"
   },
   "source": [
    "## Step 10: Quick Inference Test\n",
    "\n",
    "Test your DPO model on a sample question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_code"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(\"üß™ Loading DPO model for testing...\\n\")\n",
    "\n",
    "model_path = \"outputs/dpo-llama32-1b-mcq-merged\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"‚úì Model loaded!\\n\")\n",
    "\n",
    "# Sample question\n",
    "prompt = \"\"\"Answer the following question with step-by-step reasoning.\n",
    "\n",
    "Question: Where would you find a jellyfish that has not been captured?\n",
    "Options:\n",
    "A. ocean\n",
    "B. store\n",
    "C. tank\n",
    "D. internet\n",
    "E. aquarium\n",
    "\n",
    "Think through this step by step, then provide your answer as \"Answer: X\".\"\"\"\n",
    "\n",
    "print(\"üìù Sample Question:\")\n",
    "print(prompt)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ü§ñ Model Response:\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Generate\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=150,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Extract only the response (remove prompt)\n",
    "response_only = response[len(prompt):].strip()\n",
    "print(response_only)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\n‚úì Inference test completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## üéâ Training Complete!\n",
    "\n",
    "### What You've Accomplished:\n",
    "\n",
    "‚úÖ Loaded ECQA dataset (~10k samples)\n",
    "‚úÖ Fine-tuned Llama 3.2 1B with QLoRA (SFT)\n",
    "‚úÖ Applied Direct Preference Optimization (DPO)\n",
    "‚úÖ Evaluated all models and compared performance\n",
    "\n",
    "### Your Models:\n",
    "- `outputs/sft-llama32-1b-mcq-merged/` - SFT model\n",
    "- `outputs/dpo-llama32-1b-mcq-merged/` - DPO model (best)\n",
    "\n",
    "### Next Steps:\n",
    "1. Check your **wandb dashboard** for training metrics\n",
    "2. Review evaluation results above\n",
    "3. Test with your own questions\n",
    "4. Download models if needed (see cell below)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## Optional: Download Trained Models\n",
    "\n",
    "Download models to your local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_code"
   },
   "outputs": [],
   "source": [
    "# Zip models (this may take a few minutes)\n",
    "print(\"üì¶ Zipping trained models...\\n\")\n",
    "\n",
    "!zip -r trained_models.zip outputs/*-merged/\n",
    "\n",
    "print(\"\\n‚úì Models zipped!\")\n",
    "print(\"\\nüì• Downloading...\")\n",
    "\n",
    "from google.colab import files\n",
    "files.download('trained_models.zip')\n",
    "\n",
    "print(\"‚úì Download started! Check your browser downloads.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utilities"
   },
   "source": [
    "## üõ†Ô∏è Utilities\n",
    "\n",
    "Helpful commands for debugging and monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "util_gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU memory usage\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "util_clear"
   },
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"üßπ GPU memory cleared!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "util_status"
   },
   "outputs": [],
   "source": [
    "# Check project status\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üìä Project Status\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "checks = [\n",
    "    (\"DPO data\", \"data/dpo_pairs.jsonl\"),\n",
    "    (\"SFT model\", \"outputs/sft-llama32-1b-mcq-merged\"),\n",
    "    (\"DPO model\", \"outputs/dpo-llama32-1b-mcq-merged\"),\n",
    "]\n",
    "\n",
    "for name, path in checks:\n",
    "    if Path(path).exists():\n",
    "        print(f\"‚úì {name}\")\n",
    "    else:\n",
    "        print(f\"‚úó {name}\")\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "util_disk"
   },
   "outputs": [],
   "source": [
    "# Check disk usage\n",
    "!df -h /content/drive/MyDrive/llama32-mcq-cot/\n",
    "!du -sh /content/drive/MyDrive/llama32-mcq-cot/outputs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "troubleshooting"
   },
   "source": [
    "## üêõ Troubleshooting\n",
    "\n",
    "### Out of Memory (OOM)?\n",
    "Run this cell to use smaller settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "troubleshoot_oom"
   },
   "outputs": [],
   "source": [
    "# Edit config for lower memory usage\n",
    "print(\"‚öôÔ∏è Applying memory-optimized settings...\\n\")\n",
    "\n",
    "config_edit = '''\n",
    "# Lower memory config\n",
    "data_config.train_sample_size = 5000  # Use 5k instead of 10k\n",
    "sft_config.max_seq_length = 384       # Reduce from 512\n",
    "dpo_config.max_length = 384\n",
    "model_config.lora_r = 8               # Reduce from 16\n",
    "'''\n",
    "\n",
    "print(\"Add this to configs/config.py:\")\n",
    "print(config_edit)\n",
    "print(\"\\nThen restart training from the failed step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "footer"
   },
   "source": [
    "---\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- **Project README**: Check `README.md` for detailed documentation\n",
    "- **Colab Guide**: See `COLAB_GUIDE.md` for tips and tricks\n",
    "- **Config**: Edit `configs/config.py` to customize hyperparameters\n",
    "\n",
    "## üéì Learning Outcomes\n",
    "\n",
    "You now understand:\n",
    "- ‚úÖ QLoRA (4-bit quantization + LoRA)\n",
    "- ‚úÖ Supervised Fine-Tuning (SFT)\n",
    "- ‚úÖ Direct Preference Optimization (DPO)\n",
    "- ‚úÖ Chain-of-Thought reasoning\n",
    "- ‚úÖ Model evaluation and comparison\n",
    "\n",
    "## üìù For Your CV\n",
    "\n",
    "Check the README.md file for a ready-made CV description!\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Training! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
